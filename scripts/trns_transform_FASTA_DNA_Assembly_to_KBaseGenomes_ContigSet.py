
#!/usr/bin/env python

# standard library imports
import os
import sys
import traceback
import argparse
import json
import logging
import io
import re
import hashlib
import pprint

# 3rd party imports
import requests

# KBase imports
import biokbase.Transform.script_utils as script_utils


# conversion method that can be called if this module is imported
# Note the logger has different levels it could be run.  See: https://docs.python.org/2/library/logging.html#logging-levels
# The default level is set to INFO which includes everything except DEBUG
def convert(shock_service_url, handle_service_url, input_file_name, output_file_name, working_directory, shock_id, handle_id, fasta_reference_only, level=logging.INFO, logger=None):
    """
    Converts FASTA file to KBaseGenomes.ContigSet json string.  
    Note the MD5 for the contig is generated by uppercasing the sequence.
    The ContigSet MD5 is generated by taking the MD5 of joining the sorted list of individual contig's MD5s with a comma separator

    Args:
        shock_service_url: A url for the KBase SHOCK service.
        handle_service_url: A url for the KBase Handle Service.
        input_file_name: A file name for the input FASTA data.
        output_file_name: A file name where the output JSON string should be stored.  If the output file name is not specified the name wil\
l default to the name of the input file appended with "_contig_set"'
        working_directory: The direcotry the resulting json file will be written to.
        shock_id: Shock id for the fasta file if it already exists in shock
        handle_id: Handle id for the fasta file if it already exists as a handle
        fasta_reference_only: Creates a reference to the fasta file in Shock, but does not store the sequences in the workspace object.  Not recommended unless the fasta file is larger than 1GB. This is the default behavior for files that large.
        level: Logging level, defaults to logging.INFO.
    """

    if logger is None:
        logger = script_utils.getStderrLogger(__file__)
    
    logger.info("Starting conversion of FASTA to KBaseGenomes.ContigSet")
    token = os.environ.get('KB_AUTH_TOKEN')
    
    logger.info("Building Object.")
 
    if not os.path.isfile(input_file_name):
        raise Exception("The input file name {0} does not exist".format(input_file_name))        

    if not os.path.isdir(args.working_directory):
        raise Exception("The working directory does not exist {0} does not exist".format(working_directory))        

    # default if not too large
    contig_set_has_sequences = True 
    if fasta_reference_only:
        contig_set_has_sequences = False 

    fasta_filesize = os.stat(input_file_name).st_size
    if fasta_filesize > 1000000000:
        # Fasta file too large to save sequences into the ContigSet object.        
        logger.warning('Fasta input file {0} is be too large, the contigset will not contain sequences'.format(input_file_name)) 
        contig_set_has_sequences = False 

    input_file_handle = open(input_file_name, 'r')
    fasta_header = None
    sequence_list = []
    fasta_dict = dict()
    first_header_found = False
    contig_set_md5_list = []
    # Pattern for replacing white space
    pattern = re.compile(r'\s+')
    sequence_exists = False
    for current_line in input_file_handle:
        if (current_line[0] == ">"):
            # found a header line
            # Wrap up previous fasta sequence
            if (not sequence_exists) and first_header_found:
                logger.error("There is no sequence related to fasta record : {0}".format(fasta_header))        
                raise Exception("There is no sequence related to fasta record : {0}".format(fasta_header))    
            if not first_header_found:
                first_header_found = True
            else:
                # build up sequence and remove all white space
                total_sequence = ''.join(sequence_list)
                total_sequence = re.sub(pattern, '', total_sequence)
                fasta_key = fasta_header.strip()
                contig_dict = dict() 
                contig_dict["id"] = fasta_key 
                contig_dict["length"] = len(total_sequence) 
                contig_dict["name"] = fasta_key 
                contig_dict["description"] = "Note MD5 is generated from uppercasing the sequence" 
                contig_md5 = hashlib.md5(total_sequence.upper()).hexdigest() 
                contig_dict["md5"]= contig_md5 
                contig_set_md5_list.append(contig_md5) 
                if contig_set_has_sequences: 
                    contig_dict["sequence"]= total_sequence
                else: 
                    contig_dict["sequence"]= "" 
                fasta_dict[fasta_key] = contig_dict
               
                # get set up for next fasta sequence
                sequence_list = []
                sequence_exists = False
            fasta_header = current_line.replace('>','')
        else:
            sequence_list.append(current_line)
            sequence_exists = True

    # wrap up last fasta sequence
    if (not sequence_exists) and first_header_found: 
        logger.error("There is no sequence related to fasta record : {0}".format(fasta_header))        
        raise Exception("There is no sequence related to fasta record : {0}".format(fasta_header)) 
    else: 
        # build up sequence and remove all white space      
        total_sequence = ''.join(sequence_list)
        total_sequence = re.sub(pattern, '', total_sequence)
        fasta_key = fasta_header.strip()
        contig_dict = dict()
        contig_dict["id"] = fasta_key 
        contig_dict["length"] = len(total_sequence)
        contig_dict["name"] = fasta_key
        contig_dict["description"] = "Note MD5 is generated from uppercasing the sequence" 
        contig_md5 = hashlib.md5(total_sequence.upper()).hexdigest()
        contig_dict["md5"]= contig_md5
        contig_set_md5_list.append(contig_md5)
        if contig_set_has_sequences: 
            contig_dict["sequence"]= total_sequence 
        else:
            contig_dict["sequence"]= "" 
        fasta_dict[fasta_key] = contig_dict 
    
    contig_set_dict = dict()
    contig_set_dict["md5"]=hashlib.md5(",".join(sorted(contig_set_md5_list))).hexdigest()
    contig_set_dict["id"]=output_file_name
    contig_set_dict["name"]=output_file_name
    contig_set_dict["source"]="KBase"
    contig_set_dict["source_id"]=os.path.basename(input_file_name) 
    contig_set_dict["contigs"]= [fasta_dict[x] for x in sorted(fasta_dict.keys())]

    if shock_id is None:
        shock_id = script_utils.upload_file_to_shock(logger, shock_service_url, input_file_name, token)
    contig_set_dict["fasta_ref"]=shock_id

    # For future development if the type is updated to the handle_reference instead of a shock_reference

    # This generates the json for the object
    objectString = json.dumps(contig_set_dict, sort_keys=True, indent=4)

    logger.info("ContigSet data structure creation completed.  Writing out JSON.")
    output_file =  os.path.join(working_directory,output_file_name) 
    with open(output_file, "w") as outFile:
        outFile.write(objectString)
    
    logger.info("Conversion completed.")


# called only if script is run from command line
if __name__ == "__main__":	
    parser = argparse.ArgumentParser(prog='trns_transform_FASTA_DNA_Assembly_to_KBaseGenomes_ContigSet', 
                                     description='Converts FASTA file of assembled DNA sequences to a KBaseGenomes.ContigSet json string.',
                                     epilog='Authors: Jason Baumohl, Matt Henderson')
    # The following 7 arguments should be standard to all uploaders
    parser.add_argument('--shock_service_url', help='Shock url', action='store', type=str, nargs='?', required=True)
    parser.add_argument('--handle_service_url', help='Handle service url', action='store', type=str, nargs='?', required=False)
    parser.add_argument('--input_file_name', help ='Input Fasta file name', action='store', type=str, nargs='?', required=True)
    parser.add_argument('--working_directory', help ='Directory the output file(s) should be written into', action='store', type=str, nargs='?', required=True)
    parser.add_argument('--output_file_name', help ='Output file name for the json representation of the ContigSet.  If the output file name is not specified the name will default to the name of the input file appended with "_contig_set"', action='store', type=str, nargs='?', required=False)
    parser.add_argument('--shock_id', help='Shock node id if the fasta file already exists in shock', action='store', type=str, nargs='?', required=False)
    parser.add_argument('--handle_id', help='Handle id if the fasta file exists as a handle', action='store', type=str, nargs='?', required=False)

    # Example of a custom argument specific to this uploader
    parser.add_argument('--fasta_reference_only', help='Creates a reference to the fasta file in Shock, but does not store the sequences in the workspace object.  Not recommended unless the fasta file is larger than 1GB. This is the default behavior for files that large.', action='store_true', required=False)

    args = parser.parse_args()

    output_file_name = None
    if (args.output_file_name):
        output_file_name = args.output_file_name
    else:
        # default to input file name minus file extenstion adding "_contig_set" to the end
        base=os.path.basename(args.input_file_name)
        output_file_name = "{0}_contig_set".format(os.path.splitext(base)[0])

    logger = script_utils.getStderrLogger(__file__)
    try:
        convert(args.shock_service_url, args.handle_service_url, args.input_file_name, output_file_name, args.working_directory, args.shock_id, args.handle_id, args.fasta_reference_only,logger=logger)
    except:
        logger.exception("".join(traceback.format_exc()))
        print "".join(traceback.format_exc())
        sys.exit(1)
    
    sys.exit(0)

